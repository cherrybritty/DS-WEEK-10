# -*- coding: utf-8 -*-
"""DS Week 10

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lWONI2KX9B0GhnMCQEFLSSzF0I3q6HIm
"""

!git clone https://github.com/cherrybritty/DS-WEEK-10.git

import re
import pandas as pd
import numpy as np

# Use the 'on_bad_lines' parameter to skip or handle bad lines
week_10_data = pd.read_csv('/content/drive/MyDrive/Data set /parsed_job_data.csv', on_bad_lines='skip', delimiter=';')

# making a copy of the dataframe

week10_copy = week_10_data.copy()

# Create a new column for the minimum number of employees
def extract_min_employees(overview):
    match = re.search(r'(\d+)(?:\s*-\s*\d+|\s*\+?)\s*employees', str(overview), re.IGNORECASE)
    return int(match.group(1)) if match else None

week10_copy['employees'] = week10_copy['Company_Overview'].apply(extract_min_employees)

# Function to extract sector from company overview using keywords
def extract_sector(overview):
    sectors = ['technology', 'healthcare', 'finance', 'education', 'retail', 'automotive']
    overview_lower = str(overview).lower()
    for sector in sectors:
        if sector in overview_lower:
            return sector.capitalize()
    return 'Other'

week10_copy['sector'] = week10_copy['Company_Overview'].apply(extract_sector)

# Create a new column for the year the company was founded
def extract_year_founded(overview):
    match = re.search(r'(\d{4})', str(overview))
    return int(match.group(1)) if match else None

week10_copy['founded'] = week10_copy['Company_Overview'].apply(extract_year_founded)

def extract_salary_range(salary_fork):
    # Convert salary_fork to a string
    salary_fork = str(salary_fork)
    match = re.findall(r'(\d+)', salary_fork.replace(',', ''))
    if len(match) == 2:
        min_salary = int(match[0])
        max_salary = int(match[1])
        avg_salary = (min_salary + max_salary) / 2
        return [min_salary, max_salary, avg_salary]
    else:
        return [np.nan, np.nan, np.nan]

week10_copy[['min_salary', 'max_salary', 'avg_salary($)']] = week10_copy['Salary_fork'].apply(
    lambda x: pd.Series(extract_salary_range(x))
)

week10_copy['salary_range($)'] = week10_copy.apply(
    lambda row: f"{int(row['min_salary'])}-{int(row['max_salary'])}"
    if pd.notnull(row['min_salary']) and pd.notnull(row['max_salary'])
    else np.nan,
    axis=1
)

week10_copy.dropna(inplace=True)

# Round the ratings to the nearest integer
week10_copy['rating'] = pd.to_numeric(week10_copy['Rating'], errors='coerce').round().astype('Int64')

# Ensuring minimum length of each job description is at least 5% of the maximum job description length
max_length = week10_copy['Job_description'].str.len().max()
min_length_threshold = int(0.05 * max_length)

week10_copy['job_description'] = week10_copy['Job_description'].apply(lambda desc: desc if len(str(desc)) >= min_length_threshold else None)

# Remove words shorter than 3 characters from job descriptions
def clean_job_description(description):
    words = str(description).split()
    cleaned_words = [word for word in words if len(word) >= 3]
    return ' '.join(cleaned_words)

week10_copy['job_description'] = week10_copy['job_description'].apply(clean_job_description)

week10_copy.drop(columns=['Company_Overview', 'Salary_fork', 'Job_description', 'Avg_base_salary', 'min_salary', 'max_salary', 'employees', 'Rating'], inplace=True)

"""**SECTION** **B**"""

pip install wordcloud

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet, stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# removing special characters

def remove_special_characters(text):
    return re.sub(r'[^A-Za-z\s]', '', text)

week10_copy['character_clean'] = week10_copy['job_description'].apply(remove_special_characters)

# converting to lower case for consistency

week10_copy['character_clean'] = week10_copy['character_clean'].apply(lambda x: x.lower())

nltk.download('punkt')

week10_copy['tokenized_text'] = week10_copy['character_clean'].apply(word_tokenize)

# lemmatization

nltk.download('wordnet')
nltk.download('omw-1.4')
lemmatizer = WordNetLemmatizer()
week10_copy['lemmatized_text'] = week10_copy['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

# removal of stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
week10_copy['lemmatized_text'] = week10_copy['lemmatized_text'].apply(lambda x: [word for word in x if word not in stop_words])

# joining all the words in a row to be a sentence
# joining all the words in the column to be one long sentence sparated by an empty space

merged_words = ' '.join([' '.join(words) for words in week10_copy['lemmatized_text']])

wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, min_word_length=5, min_font_size=10).generate(merged_words)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the length of each job description

week10_copy['job_description_length'] = week10_copy['job_description'].apply(len)

# Create the histogram that focus on job description length
plt.figure(figsize=(10, 6))
sns.histplot(week10_copy['job_description_length'], kde=True, bins=15)

plt.title('Distribution of Job Description Lengths')
plt.xlabel('Job Description Length (Number of Characters)')
plt.ylabel('Frequency')
plt.show()